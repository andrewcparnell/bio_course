---
title: 'Class 2: Regression and classification'
author: "Andrew Parnell, School of Mathematics and Statistics, University College Dublin"
output:
  beamer_presentation:
    includes:
      in_header: header.tex

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, dev = 'pdf', fig.align='center', fig.width= 5, fig.height=3.5, global.par=TRUE)
par(mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01,las=1)
```

## Learning outcomes

- Be able to understand the structure of regression and classification models
- Know how to read and interpret the output of a statistical model
- Be familiar with some of the extensions to basic regression and classification models

## Why regression and classification?

- t-tests are only really useful when you have a continuous outcome variable and one discrete variable with two  groups (e.g. treatment vs control)
- For almost any real life situation you have multiple variables of all different types 
- For these situations you need a _statistical model_
- A statistical model allows to perform _probabilistic prediction_ of the outcome variable from the remaining variable, and/or to explain how the other variables are causing the outcome variable to change

## Regression vs Classification: whatâ€™s the difference?

- In regression we have a single _continuous_ outcome variable and lots of other variables which we think might be causing the outcome to change
- In classification we have a single _discrete_ outcome variable and lots of other variables
- In the machine learning literature this is often known as _supervised learning_
- Situations where there are multiple outcome variables are beyond the scope of this course

## Response and explanatory variables

- The outcome variable is more commonly known as the _response_ variable
- The other variables which we think might be causing the response variable to change are called the _explanatory variables_ (though be careful with causation)
- We will use these words from now on, but beware there are lots of other terms in the literature

## A basic regression model

- Let's go back to the prostate cancer data
- Recall the key outcome variable is `lpsa` the log of the prostate specific antigen value. This is our response variable
- Suppose we had one explanatory variable `lweight`
```{r, echo = FALSE, fig.height = 2, fig.width = 4, fig.align='center'}
prostate = read.csv('https://raw.githubusercontent.com/andrewcparnell/bio_course/master/code/prostate.csv')
par(mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01,las=1)
plot(prostate$lweight, prostate$lpsa, xlab = 'log(weight)', ylab= 'log(psa)')
```

## Creating the model

- Looking at the plot, there may be a positive, linear relationship between log(weight) and log(psa)
- Perhaps we can create a prediction model that allows us to predict log(psa) from log(weight)
- Suppose, for each patient we multiplied the log(weight) value by 1.2 and then subtracted the value 2 so:
$$prediction = 1.2 \times \log(weight) - 2$$
- If we do this repeatedly for every value in the data set we get ...

## A first model

```{r, fig.height = 2, fig.width = 4, fig.align='center'}
par(mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01,las=1)
prediction = 1.2 * prostate$lweight - 2
plot(prostate$lweight, prostate$lpsa, xlab = 'log(weight)', ylab= 'log(psa)')
points(prostate$lweight, prediction, col='red')
```

## Refining the model

- Is this model any good? 
- How might we measure how close our predictions are to the truth? 
- How can we choose the values (here 1.2 and -2) better?

## Getting R to do the work

- Luckily the R function `lm` will do the work for us
```{r}
model_1 = lm(formula = lpsa ~ lweight, data = prostate)
summary(model_1)
```

## Background details

- The two values here are the $y$-intercept and the slope of the line
- R chooses these values by minimising the vertical distances between the black and the red points (called least squares)
- A key assumption in the model is that these vertical distances (known as _residuals_) are normally distributed
- R uses this assumption to run t-tests on the parameters, which you can see the results of in the `summary` output

## Plotting the fit

- One way is to type `plot(model_1)` which gives residual diagnostics. A quick plot of the fitted line:
```{r}
par(mar=c(3,3,2,1), mgp=c(2,.7,0), tck=-.01,las=1)
plot(prostate$lweight, prostate$lpsa, xlab = 'log(weight)', ylab= 'log(psa)')
abline(model_1, col='red')
```

## Expanding the model with two explanatory variables

- Suppose we wanted to use two explanatory variables, `lweight` and `age`:
```{r}
model_2 = lm(formula = lpsa ~ lweight + age, data = prostate)
summary(model_2)
```

## Expanding the fit even more

```{r}
model_3 = lm(formula = lpsa ~ . - train, data = prostate)
summary(model_3)
```

## Multiple regression

- When you have lots of explanatory variables this is known as _multiple regression_
- You can still use the values in the `Estimate` column to create predictions of `lpsa` by multiplying and adding up
- Beware the p-values as before: they might be highly dsignificant but still a very poor model
- R gives you two other useful statistics: 
    - The R-squared which measures the proportion of variation in the repsonse variable explained by the explanatory variables
    - The residual standard error which measures how far away the data points are from the fitted line

## Dealing with interactions

- Interactions are important; our explanatory variables will often interact with each other to affect the response variable
- The usual way to deal with interactions is to create _new explanatory variables_ by multiplying them together
```{r}
model_4 = lm(formula = lpsa ~ lweight + age + lweight:age, data = prostate)
summary(model_4)
```

## Regularisation and shrinkage

- When you have lots and lots of explanatory variables, the model can become very slow or might not fit at all
- Worse, we might have lots of spurious p-values
- It makes sense to remove or reduce some of the coefficients on the explanatory variables if we think their effect is over-stated
- One way of doing this is via _regularisation_, where we set some of the values to zero, another is via _shrinkage_ where we reduce the values (shrink them towards zero)

## Lasso and Ridge

- The R package `glmnet` will perform shrinkage and regularisation
- The _Lasso_ model imposes a restricted sum on all of the coefficient values
- The _Ridge_ model imposes an extra assumption that all of the coefficient values come from a normal distribution
- We will play with some of these models later

## Even more advanced regression approaches

- There is lots of research on regression models of all different types
- The vast majority of them involve creating a set of coefficients to multiply the explanatory variables by and then adding everything up
- It's very important to check the model diagnostics

## Intro to classification models
## The logit transformation
## Example: SA Heart rate
## Extending the model
## Understanding the output
## Plotting the fitted model
## Regularisation and shrinkage for classification
## More advanced classification approaches